here::here()
nba <- na.omit(nba)
Modelo1reg <- lm(Salary ~ Age + AST_ + BLK_ + BPM + FTr + G + MP +
NBA_DraftNumber + PER + STL_ + TOV_ + TRB_ + TS_ + USG_ +
VORP + WS  + X3PAr, data = nba)
summary(Modelo1reg)
nba <- select(nba, -Player, -Tm, -NBA_Country, -ORB_, -WinSH, -OWS, -OBPM, -DRB_, -DWS, -DBPM)
round(cor(nba),2)
correlacion <- round(cor(nba), 1)
corrplot(correlacion, method = "number", type = "full")
set.seed(200)
nba_split <- initial_split(nba, prop = .7, strata = "Salary")
nba_train <- training(nba_split)
nba_test  <- testing(nba_split)
nba_train_x <- model.matrix(Salary ~ Age + AST_ + BLK_ + BPM + FTr + G + MP +
NBA_DraftNumber + PER + STL_ + TOV_ + TRB_ + TS_ + USG_ +
VORP + WS  + X3PAr, data = nba_train)[, -1]
nba_train_y <- log(nba_train$Salary)
nba_test_x <- model.matrix(Salary ~ ., nba_test)[, -1]
nba_test_y <- log(nba_test$Salary)
dim(nba_train_x)
metodo_lasso    <- glmnet(nba_train_x, nba_train_y, alpha = 1.0)
metodo_elastic1 <- glmnet(nba_train_x, nba_train_y, alpha = 0.25)
metodo_elastic2 <- glmnet(nba_train_x, nba_train_y, alpha = 0.75)
metodo_ridge    <- glmnet(nba_train_x, nba_train_y, alpha = 0.0)
par(mfrow = c(2, 2), mar = c(6, 4, 6, 2) + 0.1)
plot(metodo_lasso, xvar = "lambda", main = "Lasso (Alpha = 1)\n\n\n")
plot(metodo_elastic1, xvar = "lambda", main = "Elastic Net (Alpha = .25)\n\n\n")
plot(metodo_elastic2, xvar = "lambda", main = "Elastic Net (Alpha = .75)\n\n\n")
plot(metodo_ridge, xvar = "lambda", main = "Ridge (Alpha = 0)\n\n\n")
fold_id <- sample(1:10, size = length(nba_train_y), replace = TRUE)
nba_grid <- tibble::tibble(
alpha      = seq(0, 1, by = .1),
mse_min    = NA,
mse_1se    = NA,
lambda_min = NA,
lambda_1se = NA
)
for (i in seq_along(nba_grid$alpha)) {
nba_fit <- cv.glmnet(nba_train_x, nba_train_y, alpha = nba_grid$alpha[i], foldid = fold_id)
nba_grid$mse_min[i]    <- nba_fit$cvm[nba_fit$lambda == nba_fit$lambda.min]
nba_grid$mse_1se[i]    <- nba_fit$cvm[nba_fit$lambda == nba_fit$lambda.1se]
nba_grid$lambda_min[i] <- nba_fit$lambda.min
nba_grid$lambda_1se[i] <- nba_fit$lambda.1se
}
nba_grid
nba_grid %>%
mutate(se = mse_1se - mse_min) %>%
ggplot(aes(alpha, mse_min)) +
geom_line(size = 2) +
geom_ribbon(aes(ymax = mse_min + se, ymin = mse_min - se), alpha = 0.3) +
ggtitle("MSE ± one standard error")
cv_lasso   <- cv.glmnet(nba_train_x, nba_train_y, alpha = 1.0)
min(cv_lasso$cvm)
cv_lasso
nba_pred <- predict(cv_lasso, s = cv_lasso$lambda.min, nba_test_x)
mean((nba_test_y - nba_pred)^2)
nba_pred
library(caret)
train_control <- trainControl(method = "cv", number = 10)
caret_mod <- train(
x = nba_train_x,
y = nba_train_y,
method = "glmnet",
preProc = c("center", "scale", "zv", "nzv"),
trControl = train_control,
tuneLength = 10
)
caret_mod
library(rsample)
library(glmnet)
library(dplyr)
library(ggplot2)
nba <- read.csv("C:/Users/Equipo/Desktop/CUNEF/Prediccion/Datos/nba.csv")
here::here()
nba <- na.omit(nba)
Modelo1reg <- lm(Salary ~ Age + AST_ + BLK_ + BPM + FTr + G + MP +
NBA_DraftNumber + PER + STL_ + TOV_ + TRB_ + TS_ + USG_ +
VORP + WS  + X3PAr, data = nba)
summary(Modelo1reg)
library(rsample)
library(glmnet)
library(dplyr)
library(ggplot2)
nba <- read.csv("C:/Users/Equipo/Desktop/CUNEF/Prediccion/Datos/nba.csv")
here::here()
nba <- na.omit(nba)
Modelo1reg <- lm(Salary ~ Age + AST_ + BLK_ + BPM + FTr + G + MP +
NBA_DraftNumber + PER + STL_ + TOV_ + TRB_ + TS_ + USG_ +
VORP + WS  + X3PAr, data = nba)
summary(Modelo1reg)
nba <- select(nba, -Player, -Tm, -NBA_Country, -ORB_, -WinSH, -OWS, -OBPM, -DRB_, -DWS, -DBPM)
round(cor(nba),2)
correlacion <- round(cor(nba), 1)
corrplot(correlacion, method = "number", type = "full")
set.seed(200)
nba_split <- initial_split(nba, prop = .7, strata = "Salary")
nba_train <- training(nba_split)
nba_test  <- testing(nba_split)
nba_train_x <- model.matrix(Salary ~ Age + AST_ + BLK_ + BPM + FTr + G + MP +
NBA_DraftNumber + PER + STL_ + TOV_ + TRB_ + TS_ + USG_ +
VORP + WS  + X3PAr, data = nba_train)[, -1]
nba_train_y <- log(nba_train$Salary)
nba_test_x <- model.matrix(Salary ~ ., nba_test)[, -1]
nba_test_y <- log(nba_test$Salary)
dim(nba_train_x)
metodo_lasso    <- glmnet(nba_train_x, nba_train_y, alpha = 1.0)
metodo_elastic1 <- glmnet(nba_train_x, nba_train_y, alpha = 0.25)
metodo_elastic2 <- glmnet(nba_train_x, nba_train_y, alpha = 0.75)
metodo_ridge    <- glmnet(nba_train_x, nba_train_y, alpha = 0.0)
par(mfrow = c(2, 2), mar = c(6, 4, 6, 2) + 0.1)
plot(metodo_lasso, xvar = "lambda", main = "Lasso (Alpha = 1)\n\n\n")
plot(metodo_elastic1, xvar = "lambda", main = "Elastic Net (Alpha = .25)\n\n\n")
plot(metodo_elastic2, xvar = "lambda", main = "Elastic Net (Alpha = .75)\n\n\n")
plot(metodo_ridge, xvar = "lambda", main = "Ridge (Alpha = 0)\n\n\n")
fold_id <- sample(1:10, size = length(nba_train_y), replace = TRUE)
nba_grid <- tibble::tibble(
alpha      = seq(0, 1, by = .1),
mse_min    = NA,
mse_1se    = NA,
lambda_min = NA,
lambda_1se = NA
)
for (i in seq_along(nba_grid$alpha)) {
nba_fit <- cv.glmnet(nba_train_x, nba_train_y, alpha = nba_grid$alpha[i], foldid = fold_id)
nba_grid$mse_min[i]    <- nba_fit$cvm[nba_fit$lambda == nba_fit$lambda.min]
nba_grid$mse_1se[i]    <- nba_fit$cvm[nba_fit$lambda == nba_fit$lambda.1se]
nba_grid$lambda_min[i] <- nba_fit$lambda.min
nba_grid$lambda_1se[i] <- nba_fit$lambda.1se
}
nba_grid
nba_grid %>%
mutate(se = mse_1se - mse_min) %>%
ggplot(aes(alpha, mse_min)) +
geom_line(size = 2) +
geom_ribbon(aes(ymax = mse_min + se, ymin = mse_min - se), alpha = 0.3) +
ggtitle("MSE ± one standard error")
cv_lasso   <- cv.glmnet(nba_train_x, nba_train_y, alpha = 1.0)
min(cv_lasso$cvm)
cv_lasso
nba_pred <- predict(cv_lasso, s = cv_lasso$lambda.min, nba_test_x)
mean((nba_test_y - nba_pred)^2)
nba_pred
library(caret)
train_control <- trainControl(method = "cv", number = 10)
caret_mod <- train(
x = nba_train_x,
y = nba_train_y,
method = "glmnet",
preProc = c("center", "scale", "zv", "nzv"),
trControl = train_control,
tuneLength = 10
)
caret_mod
nba <- read.csv("C:/Users/Equipo/Desktop/CUNEF/Prediccion/Datos/nba.csv")
here::here()
nba <- na.omit(nba)
Modelo1reg <- lm(Salary ~ Age + AST_ + BLK_ + BPM + FTr + G + MP +
NBA_DraftNumber + PER + STL_ + TOV_ + TRB_ + TS_ + USG_ +
VORP + WS  + X3PAr, data = nba)
summary(Modelo1reg)
#Aqui lo que he hecho ha sido quitar las variables que no quiero usar.
nba <- select(nba, -Player, -Tm, -NBA_Country, -ORB_, -WinSH, -OWS, -OBPM, -DRB_, -DWS, -DBPM)
#MATRIZ DE CORRELACION
round(cor(nba),2)
correlacion <- round(cor(nba), 1)
corrplot(correlacion, method = "number", type = "full")
library(rsample)
library(glmnet)
library(dplyr)
library(ggplot2)
library(corrplot)
nba <- read.csv("C:/Users/Equipo/Desktop/CUNEF/Prediccion/Datos/nba.csv")
here::here()
nba <- na.omit(nba)
Modelo1reg <- lm(Salary ~ Age + AST_ + BLK_ + BPM + FTr + G + MP +
NBA_DraftNumber + PER + STL_ + TOV_ + TRB_ + TS_ + USG_ +
VORP + WS  + X3PAr, data = nba)
summary(Modelo1reg)
nba <- select(nba, -Player, -Tm, -NBA_Country, -ORB_, -WinSH, -OWS, -OBPM, -DRB_, -DWS, -DBPM)
round(cor(nba),2)
correlacion <- round(cor(nba), 1)
corrplot(correlacion, method = "number", type = "full")
set.seed(200)
nba_split <- initial_split(nba, prop = .7, strata = "Salary")
nba_train <- training(nba_split)
nba_test  <- testing(nba_split)
nba_train_x <- model.matrix(Salary ~ Age + AST_ + BLK_ + BPM + FTr + G + MP +
NBA_DraftNumber + PER + STL_ + TOV_ + TRB_ + TS_ + USG_ +
VORP + WS  + X3PAr, data = nba_train)[, -1]
nba_train_y <- log(nba_train$Salary)
nba_test_x <- model.matrix(Salary ~ ., nba_test)[, -1]
nba_test_y <- log(nba_test$Salary)
dim(nba_train_x)
metodo_lasso    <- glmnet(nba_train_x, nba_train_y, alpha = 1.0)
metodo_elastic1 <- glmnet(nba_train_x, nba_train_y, alpha = 0.25)
metodo_elastic2 <- glmnet(nba_train_x, nba_train_y, alpha = 0.75)
metodo_ridge    <- glmnet(nba_train_x, nba_train_y, alpha = 0.0)
par(mfrow = c(2, 2), mar = c(6, 4, 6, 2) + 0.1)
plot(metodo_lasso, xvar = "lambda", main = "Lasso (Alpha = 1)\n\n\n")
plot(metodo_elastic1, xvar = "lambda", main = "Elastic Net (Alpha = .25)\n\n\n")
plot(metodo_elastic2, xvar = "lambda", main = "Elastic Net (Alpha = .75)\n\n\n")
plot(metodo_ridge, xvar = "lambda", main = "Ridge (Alpha = 0)\n\n\n")
fold_id <- sample(1:10, size = length(nba_train_y), replace = TRUE)
nba_grid <- tibble::tibble(
alpha      = seq(0, 1, by = .1),
mse_min    = NA,
mse_1se    = NA,
lambda_min = NA,
lambda_1se = NA
)
for (i in seq_along(nba_grid$alpha)) {
nba_fit <- cv.glmnet(nba_train_x, nba_train_y, alpha = nba_grid$alpha[i], foldid = fold_id)
nba_grid$mse_min[i]    <- nba_fit$cvm[nba_fit$lambda == nba_fit$lambda.min]
nba_grid$mse_1se[i]    <- nba_fit$cvm[nba_fit$lambda == nba_fit$lambda.1se]
nba_grid$lambda_min[i] <- nba_fit$lambda.min
nba_grid$lambda_1se[i] <- nba_fit$lambda.1se
}
nba_grid
nba_grid %>%
mutate(se = mse_1se - mse_min) %>%
ggplot(aes(alpha, mse_min)) +
geom_line(size = 2) +
geom_ribbon(aes(ymax = mse_min + se, ymin = mse_min - se), alpha = 0.3) +
ggtitle("MSE ± one standard error")
cv_lasso   <- cv.glmnet(nba_train_x, nba_train_y, alpha = 1.0)
min(cv_lasso$cvm)
cv_lasso
nba_pred <- predict(cv_lasso, s = cv_lasso$lambda.min, nba_test_x)
mean((nba_test_y - nba_pred)^2)
nba_pred
library(caret)
train_control <- trainControl(method = "cv", number = 10)
caret_mod <- train(
x = nba_train_x,
y = nba_train_y,
method = "glmnet",
preProc = c("center", "scale", "zv", "nzv"),
trControl = train_control,
tuneLength = 10
)
caret_mod
set.seed(200)
library(rsample)
library(glmnet)
library(dplyr)
library(ggplot2)
nba <- read.csv("C:/Users/Equipo/Desktop/CUNEF/Prediccion/Datos/nba.csv")
here::here()
nba <- na.omit(nba)
Modelo1reg <- lm(Salary ~ Age + AST_ + BLK_ + BPM + FTr + G + MP +
NBA_DraftNumber + PER + STL_ + TOV_ + TRB_ + TS_ + USG_ +
VORP + WS  + X3PAr, data = nba)
summary(Modelo1reg)
#Aqui lo que he hecho ha sido quitar las variables que no quiero usar.
nba <- select(nba, -Player, -Tm, -NBA_Country, -ORB_, -WinSH, -OWS, -OBPM, -DRB_, -DWS, -DBPM)
#MATRIZ DE CORRELACION
round(cor(nba),2)
correlacion <- round(cor(nba), 1)
corrplot(correlacion, method = "number", type = "full")
#Al analizar la matriz de correlacion podemos observar que la variable MP esta muy
#correlacionada con la variable G, al igual que BPM con con PER y WS con VORP. Esto
#quiere decir que los valores de cada una de ellas varian sistematicamente con respecto
#a los valores de la que hemos considerado aqui su pareja, es decir existe una relacion
#directa. Como en este caso lo que queremos ver son que variables son las mas afectan
#a la hora de predecir el salario, cabe señalar que el Salario esta correlacionado
#positivamente con las variables WS y VORP en un 0.6 de coeficiente de correlacion y con la
#variable MP en un 0.5 lo que nos quiere decir que al aumentar dichas variables el Salrio
#aumenta proporcionalmente. Al contrario ocurre con la variable NBA_Draft que respecto al
#salario existe una correlacion negativa y eso es muy logico ya que a posteriores rondas de
#seleccion en el Draft el salario disminuye ya que no es lo mismo ser elegido en primera ronda
#que en rondas posteriores y mas aun cuando un jugador es elegido en como primero en la primera
#ronda, el cual es considerado el mejor "universitario" del año.
#La finalidad de usar los training ha sido porque nos sirve para ajustar el modelo.
#Gracias al metodo de Cross-Validation hemos generado los training set y el test de prueba
#con el que evaluaremos la capacidad predictiva mediante el error de prediccion.
set.seed(200)
nba_split <- initial_split(nba, prop = .7, strata = "Salary")
nba_train <- training(nba_split)
nba_test  <- testing(nba_split)
nba_train_x <- model.matrix(Salary ~ Age + AST_ + BLK_ + BPM + FTr + G + MP +
NBA_DraftNumber + PER + STL_ + TOV_ + TRB_ + TS_ + USG_ +
VORP + WS  + X3PAr, data = nba_train)[, -1]
nba_train_y <- log(nba_train$Salary)
nba_test_x <- model.matrix(Salary ~ ., nba_test)[, -1]
nba_test_y <- log(nba_test$Salary)
#Con esta matriz lo que vemos es que el numero de
#observaciones ha sido reducido con respecto a la base de datos original ya que el "train"
#ha reducido la muestra.
dim(nba_train_x)
#A partir de aqui me he dedicado a usar el metodo de regularizacion llamado Elastic Nets.
#Dicha tecnica selecciona variables automaticamente y supera la eficacia del metodo Lasso.
#Es muy util cuando el numero de estimadores es mayor que el numero de observaciones.
metodo_lasso    <- glmnet(nba_train_x, nba_train_y, alpha = 1.0)
metodo_elastic1 <- glmnet(nba_train_x, nba_train_y, alpha = 0.25)
metodo_elastic2 <- glmnet(nba_train_x, nba_train_y, alpha = 0.75)
metodo_ridge    <- glmnet(nba_train_x, nba_train_y, alpha = 0.0)
#En el paso siguiente lo que tenemos que tener en cuenta que cuanto mayor sea Lambda mayor es la penalizacion
#en los coeficientes de regresion estimados en funcion de dicho parametro y como lo mas
#optimo es elegir el valor mas pequeño de Lambda con el cual se estabilizan dichos
#coeficientes.
par(mfrow = c(2, 2), mar = c(6, 4, 6, 2) + 0.1)
plot(lasso, xvar = "lambda", main = "Lasso (Alpha = 1)\n\n\n")
plot(elastic1, xvar = "lambda", main = "Elastic Net (Alpha = .25)\n\n\n")
plot(elastic2, xvar = "lambda", main = "Elastic Net (Alpha = .75)\n\n\n")
plot(ridge, xvar = "lambda", main = "Ridge (Alpha = 0)\n\n\n")
#Llegados a este punto mediante tecnicas de Cross-Validation he elegido que el parametro k sea 10
#(aunque tambien se puede utlizar k=5) esto quiere decir que el conjunto de datos se ha divido
#en 10 subconjuntos de tamaño similar para hacer mas facil el analisis.
fold_id <- sample(1:10, size = length(nba_train_y), replace = TRUE)
nba_grid <- tibble::tibble(
alpha      = seq(0, 1, by = .1),
mse_min    = NA,
mse_1se    = NA,
lambda_min = NA,
lambda_1se = NA
)
for (i in seq_along(nba_grid$alpha)) {
nba_fit <- cv.glmnet(nba_train_x, nba_train_y, alpha = nba_grid$alpha[i], foldid = fold_id)
nba_grid$mse_min[i]    <- nba_fit$cvm[nba_fit$lambda == nba_fit$lambda.min]
nba_grid$mse_1se[i]    <- nba_fit$cvm[nba_fit$lambda == nba_fit$lambda.1se]
nba_grid$lambda_min[i] <- nba_fit$lambda.min
nba_grid$lambda_1se[i] <- nba_fit$lambda.1se
}
nba_grid
#El glmnet es un de estimacion denominado Modelo Lineal en el cual los estimadores estan
#generalizados.
nba_grid %>%
mutate(se = mse_1se - mse_min) %>%
ggplot(aes(alpha, mse_min)) +
geom_line(size = 2) +
geom_ribbon(aes(ymax = mse_min + se, ymin = mse_min - se), alpha = 0.3) +
ggtitle("MSE ± one standard error")
#Realiza el test de Cramer-Von Mises para conocer la bondad de ajuste del modelo.
cv_lasso   <- cv.glmnet(nba_train_x, nba_train_y, alpha = 1.0)
min(cv_lasso$cvm)
cv_lasso
#A su vez con este modelo he obtenido un coeficiente del error medio de validacion cruzada de
#1.136829, que es el valor de la media de los valores de error de cada uno de mis diez k.
#Con la funcion predict vamos a predecir valores ajustados y otros coeficientes.
nba_pred <- predict(cv_lasso, s = cv_lasso$lambda.min, nba_test_x)
mean((nba_test_y - nba_pred)^2)
nba_pred
#Aqui voy a utilizar el metodo Caret para contrastar.
library(caret)
train_control <- trainControl(method = "cv", number = 10)
caret_mod <- train(
x = nba_train_x,
y = nba_train_y,
method = "glmnet",
preProc = c("center", "scale", "zv", "nzv"),
trControl = train_control,
tuneLength = 10
)
caret_mod
nba_pred
View(nba_test_x)
count(nba_test_x)
mean((nba_test_y - nba_pred)^2)
#Despues de hacer el metodo Caret (utilice este porque me parecio mas practico que el H2O)
#he de decir que la lambda que me da es mucho mayor que la que obtengo con el metodo de Elastic Nets
#en un principio llegue a pensar que ambos modelos hacian lo mismo pero a reiz de los resultados,
#reconozco que me surgen dudas, porque la verdad es que hay mucha diferencia entre ambos valores de lambda.
#El alpha optimo para este modelo es 1, el cual es el valor maximo para distribuir la penalizacion. Por tanto
#me quedare con el metodo de Elastic Nets el cual me produce un modelo con un nivel muy aceptable de prediccion,
#por ello acabare diciendo que las variables elegidas predicen bien el salario a pagar a los jugadores de la NBA.
coef(cv_lasso)
library(rsample)
library(glmnet)
library(dplyr)
library(ggplot2)
nba <- read.csv("C:/Users/Equipo/Desktop/CUNEF/Prediccion/Datos/nba.csv")
nba <- na.omit(nba)
Modelo1reg <- lm(Salary ~ Age + AST_ + BLK_ + BPM + FTr + G + MP +
NBA_DraftNumber + PER + STL_ + TOV_ + TRB_ + TS_ + USG_ +
VORP + WS  + X3PAr, data = nba)
summary(Modelo1reg)
#Aqui lo que he hecho ha sido quitar las variables que no quiero usar.
nba <- select(nba, -Player, -Tm, -NBA_Country, -ORB_, -WinSH, -OWS, -OBPM, -DRB_, -DWS, -DBPM)
#MATRIZ DE CORRELACION
round(cor(nba),2)
correlacion <- round(cor(nba), 1)
corrplot(correlacion, method = "number", type = "full")
library(corrplot)
#MATRIZ DE CORRELACION
round(cor(nba),2)
correlacion <- round(cor(nba), 1)
corrplot(correlacion, method = "number", type = "full")
set.seed(200)
nba_split <- initial_split(nba, prop = .7, strata = "Salary")
nba_train <- training(nba_split)
nba_test  <- testing(nba_split)
nba_train_x <- model.matrix(Salary ~ Age + AST_ + BLK_ + BPM + FTr + G + MP +
NBA_DraftNumber + PER + STL_ + TOV_ + TRB_ + TS_ + USG_ +
VORP + WS  + X3PAr, data = nba_train)[, -1]
nba_train_y <- log(nba_train$Salary)
nba_test_x <- model.matrix(Salary ~ ., nba_test)[, -1]
nba_test_y <- log(nba_test$Salary)
dim(nba_train_x)
metodo_lasso    <- glmnet(nba_train_x, nba_train_y, alpha = 1.0)
metodo_elastic1 <- glmnet(nba_train_x, nba_train_y, alpha = 0.25)
metodo_elastic2 <- glmnet(nba_train_x, nba_train_y, alpha = 0.75)
metodo_ridge    <- glmnet(nba_train_x, nba_train_y, alpha = 0.0)
par(mfrow = c(2, 2), mar = c(6, 4, 6, 2) + 0.1)
plot(lasso, xvar = "lambda", main = "Lasso (Alpha = 1)\n\n\n")
plot(elastic1, xvar = "lambda", main = "Elastic Net (Alpha = .25)\n\n\n")
plot(elastic2, xvar = "lambda", main = "Elastic Net (Alpha = .75)\n\n\n")
plot(ridge, xvar = "lambda", main = "Ridge (Alpha = 0)\n\n\n")
fold_id <- sample(1:10, size = length(nba_train_y), replace = TRUE)
nba_grid <- tibble::tibble(
alpha      = seq(0, 1, by = .1),
mse_min    = NA,
mse_1se    = NA,
lambda_min = NA,
lambda_1se = NA
)
for (i in seq_along(nba_grid$alpha)) {
nba_fit <- cv.glmnet(nba_train_x, nba_train_y, alpha = nba_grid$alpha[i], foldid = fold_id)
nba_grid$mse_min[i]    <- nba_fit$cvm[nba_fit$lambda == nba_fit$lambda.min]
nba_grid$mse_1se[i]    <- nba_fit$cvm[nba_fit$lambda == nba_fit$lambda.1se]
nba_grid$lambda_min[i] <- nba_fit$lambda.min
nba_grid$lambda_1se[i] <- nba_fit$lambda.1se
}
nba_grid
nba_grid %>%
mutate(se = mse_1se - mse_min) %>%
ggplot(aes(alpha, mse_min)) +
geom_line(size = 2) +
geom_ribbon(aes(ymax = mse_min + se, ymin = mse_min - se), alpha = 0.3) +
ggtitle("MSE ± one standard error")
min(cv_lasso$cvm)
#Realiza el test de Cramer-Von Mises para conocer la bondad de ajuste del modelo.
cv_lasso   <- cv.glmnet(nba_train_x, nba_train_y, alpha = 1.0)
min(cv_lasso$cvm)
cv_lasso
coef(cv_lasso)
library(rsample)
library(glmnet)
library(dplyr)
library(ggplot2)
library(corrplot)
nba <- read.csv("C:/Users/Equipo/Desktop/CUNEF/Prediccion/Datos/nba.csv")
here::here()
nba <- na.omit(nba)
Modelo1reg <- lm(Salary ~ Age + AST_ + BLK_ + BPM + FTr + G + MP +
NBA_DraftNumber + PER + STL_ + TOV_ + TRB_ + TS_ + USG_ +
VORP + WS  + X3PAr, data = nba)
summary(Modelo1reg)
nba <- select(nba, -Player, -Tm, -NBA_Country, -ORB_, -WinSH, -OWS, -OBPM, -DRB_, -DWS, -DBPM)
round(cor(nba),2)
correlacion <- round(cor(nba), 1)
corrplot(correlacion, method = "number", type = "full")
set.seed(200)
nba_split <- initial_split(nba, prop = .7, strata = "Salary")
nba_train <- training(nba_split)
nba_test  <- testing(nba_split)
nba_train_x <- model.matrix(Salary ~ Age + AST_ + BLK_ + BPM + FTr + G + MP +
NBA_DraftNumber + PER + STL_ + TOV_ + TRB_ + TS_ + USG_ +
VORP + WS  + X3PAr, data = nba_train)[, -1]
nba_train_y <- log(nba_train$Salary)
nba_test_x <- model.matrix(Salary ~ ., nba_test)[, -1]
nba_test_y <- log(nba_test$Salary)
dim(nba_train_x)
metodo_lasso    <- glmnet(nba_train_x, nba_train_y, alpha = 1.0)
metodo_elastic1 <- glmnet(nba_train_x, nba_train_y, alpha = 0.25)
metodo_elastic2 <- glmnet(nba_train_x, nba_train_y, alpha = 0.75)
metodo_ridge    <- glmnet(nba_train_x, nba_train_y, alpha = 0.0)
par(mfrow = c(2, 2), mar = c(6, 4, 6, 2) + 0.1)
plot(metodo_lasso, xvar = "lambda", main = "Lasso (Alpha = 1)\n\n\n")
plot(metodo_elastic1, xvar = "lambda", main = "Elastic Net (Alpha = .25)\n\n\n")
plot(metodo_elastic2, xvar = "lambda", main = "Elastic Net (Alpha = .75)\n\n\n")
plot(metodo_ridge, xvar = "lambda", main = "Ridge (Alpha = 0)\n\n\n")
fold_id <- sample(1:10, size = length(nba_train_y), replace = TRUE)
nba_grid <- tibble::tibble(
alpha      = seq(0, 1, by = .1),
mse_min    = NA,
mse_1se    = NA,
lambda_min = NA,
lambda_1se = NA
)
for (i in seq_along(nba_grid$alpha)) {
nba_fit <- cv.glmnet(nba_train_x, nba_train_y, alpha = nba_grid$alpha[i], foldid = fold_id)
nba_grid$mse_min[i]    <- nba_fit$cvm[nba_fit$lambda == nba_fit$lambda.min]
nba_grid$mse_1se[i]    <- nba_fit$cvm[nba_fit$lambda == nba_fit$lambda.1se]
nba_grid$lambda_min[i] <- nba_fit$lambda.min
nba_grid$lambda_1se[i] <- nba_fit$lambda.1se
}
nba_grid
nba_grid %>%
mutate(se = mse_1se - mse_min) %>%
ggplot(aes(alpha, mse_min)) +
geom_line(size = 2) +
geom_ribbon(aes(ymax = mse_min + se, ymin = mse_min - se), alpha = 0.3) +
ggtitle("MSE ± one standard error")
cv_lasso   <- cv.glmnet(nba_train_x, nba_train_y, alpha = 1.0)
min(cv_lasso$cvm)
cv_lasso
coef(cv_lasso)
nba_pred <- predict(cv_lasso, s = cv_lasso$lambda.min, nba_test_x)
mean((nba_test_y - nba_pred)^2)
nba_pred
library(caret)
train_control <- trainControl(method = "cv", number = 10)
caret_mod <- train(
x = nba_train_x,
y = nba_train_y,
method = "glmnet",
preProc = c("center", "scale", "zv", "nzv"),
trControl = train_control,
tuneLength = 10
)
caret_mod
